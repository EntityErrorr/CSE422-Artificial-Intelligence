# -*- coding: utf-8 -*-
"""CSE422_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5ZPdFWQ95nkJUtENFQ1EfImZWIV1aEp
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""### Importing dependencies"""

import pandas as pd
import numpy as np
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, fbeta_score
from seaborn import heatmap
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import plotly.express as px

"""### Reading the Dataset and seeing the datapoints"""

dataset = pd.read_csv("/content/gdrive/My Drive/Dataset/fraudTrain_new.csv")
dataset.shape

dataset.info()

"""### Exploring the dataset's rows and columns"""

dataset.head()

"""### Unbalaced dataset"""

labels=["Genuine","Fraud"]

fraud_or_not = dataset["is_fraud"].value_counts().tolist()
values = [fraud_or_not[0], fraud_or_not[1]]

fig = px.pie(values=dataset['is_fraud'].value_counts(), names=labels , width=700, height=400, color_discrete_sequence=["skyblue","black"]
             ,title="Fraud vs Genuine transactions")
fig.show()

"""### Data pre-processing"""

dataset.isnull().sum()

"""###  Handling with Null values"""

impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(dataset[['amt']])
dataset['amt'] = impute.transform(dataset[['amt']])

dataset.isnull().sum()

"""###  Handling with Catagorical values"""

object_cols = dataset.select_dtypes(include=['object']).columns
for col in object_cols:
    le = LabelEncoder()
    dataset[col] = le.fit_transform(dataset[col])

dataset.head()

dataset['is_fraud'].value_counts()

"""### Correlation of all the features"""

corr = dataset.corr()
corr

sns.heatmap(corr, cmap = 'YlGnBu')

"""### Feature scaling"""

train_df = dataset.drop(["Unnamed: 0", "gender", "first", "last", "street", "city", "state", "zip", "dob","trans_num"], axis=1)

"""### Spliting the dataset"""

X = train_df.drop("is_fraud",axis=1).values
y = train_df["is_fraud"].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=2)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""### Scaling the data"""

scaler= StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""### Applying KNN on our dataset"""

knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

knn_old_training = accuracy_score(y_train, knn.predict(X_train))
knn_old_test = accuracy_score(y_test, y_pred)
print(f'Training Accuracy: {knn_old_training}')
print(f'Testing Accuracy: {knn_old_test}')

report = classification_report(y_test, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, pos_label=1)
knn_old_Precision = precision[1]
knn_old_Recall = recall[1]
print('Precision:', knn_old_Precision)
print('Recall:', knn_old_Recall)
print(f"F1: {f1[0]}")
# print(f"Support : {support[0]}")

knn_cm = confusion_matrix(y_test, y_pred)
print(knn_cm)

sns.heatmap(knn_cm, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Applying LogisticRegression on our dataset"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)

model.fit(X_train,y_train)

X_train_prediction = model.predict(X_train)
logi_old_training_data_accuracy = accuracy_score(X_train_prediction, y_train)
X_test_prediction = model.predict(X_test)
logi_old_test_data_accuracy = accuracy_score(X_test_prediction, y_test)

print('Accuracy on Training data : ', logi_old_training_data_accuracy)
print('Accuracy score on Test Data : ', logi_old_test_data_accuracy)

report = classification_report(y_test, X_test_prediction)
precision, recall, f1, support = precision_recall_fscore_support(y_test, X_test_prediction, pos_label=1)
logi_old_Precision = precision[1]
logi_old_Recall = recall[1]
print('Precision:', logi_old_Precision)
print('Recall:', logi_old_Recall)
print(f"F1: {f1[0]}")

logi_cm_old = confusion_matrix(y_test, X_test_prediction)
print(logi_cm_old)

sns.heatmap(logi_cm_old, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Applying Naive Bayes on our dataset"""

gnb = GaussianNB()
gnb.fit(X_train, y_train)

gnb_old_training = gnb.score(X_train, y_train)
gnb_old_test = gnb.score(X_test, y_test)
print("Training accuracy of the model is {:.2f}".format(gnb_old_training))
print("Testing accuracy of the model is {:.2f}".format(gnb_old_test))

y_pred = gnb.predict(X_test)
report = classification_report(y_test, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, pos_label=1)
gnb_old_Precision = precision[1]
gnb_old_Recall = recall[1]
print('Precision:', gnb_old_Precision)
print('Recall:', gnb_old_Recall)
print(f"F1: {f1[0]}")

predictions = gnb.predict(X_test)
gnb_old_new=confusion_matrix(predictions, y_test)
print(gnb_old_new)

sns.heatmap(gnb_old_new, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Over sampling the dataset"""

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

pd.Series(y_resampled).value_counts()

"""### Under sampaling the dataset"""

legit = dataset[dataset.is_fraud==0]
fraud = dataset[dataset.is_fraud==1]
print(legit.shape)
print(fraud.shape)

legit.amt.describe()

fraud.amt.describe()

dataset.groupby('is_fraud').mean()

legitSample = legit.sample(n=7506)

newDataset = pd.concat([legitSample, fraud], axis = 0)

newDataset.head()

newDataset.groupby('is_fraud').mean()

corr = newDataset.corr()
corr

sns.heatmap(corr, cmap = 'YlGnBu')

train_df2 = newDataset.drop(["Unnamed: 0", "gender", "first", "last", "street", "city", "state", "zip","dob","trans_num"], axis=1)

nX = train_df2.drop("is_fraud",axis=1)
ny = train_df2["is_fraud"]
X_trainN, X_testN, y_trainN, y_testN = train_test_split(nX, ny, test_size=0.3,stratify=ny,random_state=2)

scaler.fit(X_trainN)
print(scaler.mean_)
X_trainN = scaler.transform(X_trainN)
X_testN = scaler.transform(X_testN)

"""### Applying KNN on our under sampled dataset"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_trainN, y_trainN)
y_pred = knn.predict(X_testN)

knn_new_training = accuracy_score(y_trainN, knn.predict(X_trainN))
knn_new_test = accuracy_score(y_testN, y_pred)
print(f'Training Accuracy: {knn_new_training}')
print(f'Testing Accuracy: {knn_new_test}')

report = classification_report(y_testN, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_testN, y_pred, pos_label=1)
knn_new_Precision = precision[1]
knn_new_Recall = recall[1]
print('Precision:', knn_new_Precision)
print('Recall:', knn_new_Recall)
print(f"F1: {f1[1]}")

knn_cm_new = confusion_matrix(y_testN, y_pred)
print(knn_cm_new)

sns.heatmap(knn_cm_new, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Applying LogisticRegression on our under sampled dataset"""

model = LogisticRegression(max_iter=1000)

model.fit(X_trainN,y_trainN)

X_train_prediction = model.predict(X_trainN)
logi_new_training_data_accuracy = accuracy_score(X_train_prediction, y_trainN)
X_test_prediction = model.predict(X_testN)
logi_new_test_data_accuracy = accuracy_score(X_test_prediction, y_testN)

print('Accuracy on Training data : ', logi_new_training_data_accuracy)
print('Accuracy score on Test Data : ', logi_new_test_data_accuracy)

report = classification_report(y_testN, X_test_prediction)
precision, recall, f1, support = precision_recall_fscore_support(y_testN, X_test_prediction, pos_label=1)
logi_new_Precision = precision[1]
logi_new_Recall = recall[1]
print('Precision:', logi_new_Precision)
print('Recall:', logi_new_Recall)
print(f"F1: {f1[1]}")

logi_cm_new = confusion_matrix(y_testN, X_test_prediction)
print(logi_cm_new)

sns.heatmap(logi_cm_new, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Applying Naive Bayes on our under sampled dataset"""

gnb = GaussianNB()
gnb.fit(X_trainN, y_trainN)

gnb_new_training = gnb.score(X_trainN, y_trainN)
gnb_new_test = gnb.score(X_testN, y_testN)
print("Training accuracy of the model is {:.2f}".format(gnb_new_training))
print("Testing accuracy of the model is {:.2f}".format(gnb_new_test))

y_pred = gnb.predict(X_testN)
report = classification_report(y_testN, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_testN, y_pred, pos_label=1)
gnb_new_Precision = precision[1]
gnb_new_Recall = recall[1]
print('Precision:', gnb_new_Precision)
print('Recall:', gnb_new_Precision)

predictions = gnb.predict(X_testN)
gnb_cm_new=confusion_matrix(predictions, y_testN)
print(gnb_cm_new)

sns.heatmap(gnb_cm_new, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

"""### Accuracy Comparison of Three Models"""

accuracy = {'Model': ['knn_old', 'LogisticRegression_old', 'Gnb_old'],
        'Accuracy': [knn_old_test*100, logi_old_test_data_accuracy*100, gnb_old_test*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='red', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Accuracy Comparison of Three Models')
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy(%)')
plt.show()

"""### Accuracy Comparison of Three Models using under sampled data"""

accuracy = {'Model': ['knn_new', 'LogisticRegression_new', 'Gnb_new'],
        'Accuracy': [knn_new_test*100, logi_new_test_data_accuracy*100, gnb_new_test*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='purple', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Accuracy Comparison of Three Models using under sampled data')
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy(%)')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# First graph
accuracy1 = {'Model': ['knn_old', 'LogisticRegression_old', 'Gnb_old'],
        'Accuracy': [knn_old_test*100, logi_old_test_data_accuracy*100, gnb_old_test*100]}
df1 = pd.DataFrame(accuracy1)

# Second graph
accuracy2 = {'Model': ['knn_new', 'LogisticRegression_new', 'Gnb_new'],
        'Accuracy': [knn_new_test*100, logi_new_test_data_accuracy*100, gnb_new_test*100]}
df2 = pd.DataFrame(accuracy2)

# Set bar width and spacing
bar_width = 0.3
space = 0.2

# Set x-axis positions for bars in the second set of data
x2 = np.arange(len(df2)) + bar_width + space

# Plot merged graph
plt.bar(np.arange(len(df1)), df1['Accuracy'], width=bar_width, color='red', label='Before Under Sample')
plt.bar(x2, df2['Accuracy'], width=bar_width, color='purple', label='After Under Sample')
plt.xticks(np.arange(len(df1)), df1['Model'])
plt.ylim([0, 100])
plt.title('Accuracy Comparison of Three Models')
plt.xlabel('Models')
plt.ylabel('Accuracy(%)')
plt.legend()
plt.show()

"""### Precision Comparison of Three Models"""

accuracy = {'Model': ['knn_old', 'LogisticRegression_old', 'Gnb_old'],
        'Accuracy': [knn_old_Precision*100, logi_old_Precision*100, gnb_old_Precision*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='blue', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Precision Comparison of Three Models')
ax.set_xlabel('Models')
ax.set_ylabel('Precision(%)')
plt.show()

"""### Precision Comparison of Three Models using under sampled data"""

accuracy = {'Model': ['knn_new', 'LogisticRegression_new', 'Gnb_new'],
        'Accuracy': [knn_new_Precision*100, logi_new_Precision*100, gnb_new_Precision*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='red', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Precision Comparison of Three Models using under sampled data')
ax.set_xlabel('Models')
ax.set_ylabel('Precision(%)')
plt.show()

"""### Recall Comparison of Three Models"""

accuracy = {'Model': ['knn_old', 'LogisticRegression_old', 'Gnb_old'],
        'Accuracy': [knn_old_Recall*100, logi_old_Recall*100, gnb_old_Recall*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='blue', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Recall Comparison of Three Models')
ax.set_xlabel('Models')
ax.set_ylabel('Recall(%)')
plt.show()

"""### Recall Comparison of Three Models using under sampled data"""

accuracy = {'Model': ['knn_new', 'LogisticRegression_new', 'Gnb_new'],
        'Accuracy': [knn_new_Recall*100, logi_new_Recall*100, gnb_new_Recall*100]}
df = pd.DataFrame(accuracy)
ax = df.plot(kind='bar', x='Model', y='Accuracy', color='red', legend=False)
ax.set_ylim([0, 100])
ax.set_title('Recall Comparison of Three Models using under sampled data')
ax.set_xlabel('Models')
ax.set_ylabel('Recall(%)')
plt.show()

"""### Oversampaling the Dataset"""

from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report


method= SMOTE()
X_resampled, y_resampled = method.fit_resample(X_train, y_train)

knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_resampled, y_resampled)

y_pred = knn.predict(X_test)

knn_over_training = accuracy_score(y_resampled, knn.predict(X_resampled))
knn_over_test = accuracy_score(y_test, y_pred)
print(f'Training Accuracy: {knn_over_training}')
print(f'Testing Accuracy: {knn_over_test}')

report = classification_report(y_test, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, pos_label=1)
knn_over_Precision = precision[1]
knn_over_Recall = recall[1]
print('Precision:', knn_old_Precision)
print('Recall:', knn_old_Recall)
print(f"F1: {f1[0]}")

knn_over_cm = confusion_matrix(y_test, y_pred)
print(knn_over_cm)

sns.heatmap(knn_over_cm, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

model = LogisticRegression(max_iter=1000)

model.fit(X_resampled,y_resampled)

X_train_prediction = model.predict(X_resampled)
logi_old_training_data_accuracy = accuracy_score(X_train_prediction, y_resampled)
X_test_prediction = model.predict(X_test)
logi_old_test_data_accuracy = accuracy_score(X_test_prediction, y_test)

print('Accuracy on Training data : ', logi_old_training_data_accuracy)
print('Accuracy score on Test Data : ', logi_old_test_data_accuracy)

report = classification_report(y_test, X_test_prediction)
precision, recall, f1, support = precision_recall_fscore_support(y_test, X_test_prediction, pos_label=1)
logi_old_Precision = precision[1]
logi_old_Recall = recall[1]
print('Precision:', logi_old_Precision)
print('Recall:', logi_old_Recall)

logi_cm_old = confusion_matrix(y_test, X_test_prediction)
print(logi_cm_old)

sns.heatmap(logi_cm_old, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

gnb = GaussianNB()
gnb.fit(X_resampled, y_resampled)

gnb_old_training = gnb.score(X_resampled, y_resampled)
gnb_old_test = gnb.score(X_test, y_test)
print("Training accuracy of the model is {:.2f}".format(gnb_old_training))
print("Testing accuracy of the model is {:.2f}".format(gnb_old_test))

y_pred = gnb.predict(X_test)
report = classification_report(y_test, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, pos_label=1)
gnb_old_Precision = precision[1]
gnb_old_Recall = recall[1]
print('Precision:', gnb_old_Precision)
print('Recall:', gnb_old_Recall)

predictions = gnb.predict(X_test)
gnb_old_new=confusion_matrix(predictions, y_test)
print(gnb_old_new)

sns.heatmap(logi_cm_old, annot=True, cmap='Pastel1_r', xticklabels=['Fraud' ,'Genuine'],yticklabels=['Fraud' ,'Genuine'])

X_resampled.shape

X_test.shape

X_trainN.shape